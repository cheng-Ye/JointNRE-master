{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Kernel Anaconda2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run initial.py\n",
    "transform the format of datasets to fit in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial.py\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# folder of training datasets\n",
    "data_path = \"./origin_data/\"\n",
    "# files to export data\n",
    "export_path = \"./data/\"\n",
    "#length of sentence\n",
    "fixlen = 120\n",
    "#max length of position embedding is 100 (-100~+100)\n",
    "maxlen = 100\n",
    "\n",
    "word2id = {}\n",
    "relation2id = {}\n",
    "word_size = 0\n",
    "word_vec = None\n",
    "\n",
    "def pos_embed(x):\n",
    "\treturn max(0, min(x + maxlen, maxlen + maxlen + 1))\n",
    "\n",
    "def find_index(x,y):\n",
    "\tfor index, item in enumerate(y):\n",
    "\t\tif x == item:\n",
    "\t\t\treturn index\n",
    "\treturn -1\n",
    "\n",
    "def init_word():\n",
    "\t# reading word embedding data...\n",
    "\tglobal word2id, word_size\n",
    "\tres = []\n",
    "\tff = open(export_path + \"/entity2id.txt\", \"w\")\n",
    "\tf = open(data_path + \"kg/train.txt\", \"r\")\n",
    "\twhile True:\n",
    "\t\tcontent = f.readline()\n",
    "\t\tif content == \"\":\n",
    "\t\t\tbreak\n",
    "\t\th, t, r = content.strip().split(\"\\t\")\n",
    "\t\tif not h in word2id:\n",
    "\t\t\tword2id[h] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(h, word2id[h]))\n",
    "\t\tif not t in word2id:\n",
    "\t\t\tword2id[t] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(t, word2id[t]))\n",
    "\tf.close()\n",
    "\tf = open(data_path + \"text/train.txt\", \"r\")\n",
    "\twhile True:\n",
    "\t\tcontent = f.readline()\n",
    "\t\tif content == \"\":\n",
    "\t\t\tbreak\n",
    "\t\th,t = content.strip().split(\"\\t\")[:2]\n",
    "\t\tif not h in word2id:\n",
    "\t\t\tword2id[h] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(h, word2id[h]))\n",
    "\t\tif not t in word2id:\n",
    "\t\t\tword2id[t] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(t, word2id[t]))\n",
    "\tf.close()\n",
    "\tf = open(data_path + \"text/test.txt\", \"r\")\n",
    "\twhile True:\n",
    "\t\tcontent = f.readline()\n",
    "\t\tif content == \"\":\n",
    "\t\t\tbreak\n",
    "\t\th,t = content.strip().split(\"\\t\")[:2]\n",
    "\t\tif not h in word2id:\n",
    "\t\t\tword2id[h] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(h, word2id[h]))\n",
    "\t\tif not t in word2id:\n",
    "\t\t\tword2id[t] = len(word2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(t, word2id[t]))\n",
    "\tf.close()\n",
    "\tres.append(len(word2id))\n",
    "\tff.close()\n",
    "\n",
    "\tprint 'reading word embedding data...'\n",
    "\tf = open(data_path + 'text/vec.txt', \"r\")\n",
    "\ttotal, size = f.readline().strip().split()[:2]\n",
    "\ttotal = (int)(total)\n",
    "\tword_size = (int)(size)\n",
    "\tvec = np.ones((total + res[0], word_size), dtype = np.float32)\n",
    "\tfor i in range(total):\n",
    "\t\tcontent = f.readline().strip().split()\n",
    "\t\tword2id[content[0]] = len(word2id)\n",
    "\t\tfor j in range(word_size):\n",
    "\t\t\tvec[i + res[0]][j] = (float)(content[j+1])\n",
    "\tf.close()\n",
    "\tword2id['UNK'] = len(word2id)\n",
    "\tword2id['BLANK'] = len(word2id)\n",
    "\tglobal word_vec\n",
    "\tword_vec = vec\n",
    "\tres.append(len(word2id))\n",
    "\treturn res\n",
    "\n",
    "def init_relation():\n",
    "\t# reading relation ids...\n",
    "\tglobal relation2id\n",
    "\tprint 'reading relation ids...'\t\n",
    "\tres = []\n",
    "\tff = open(export_path + \"/relation2id.txt\", \"w\")\n",
    "\tf = open(data_path + \"text/relation2id.txt\",\"r\")\n",
    "\ttotal = (int)(f.readline().strip())\n",
    "\tfor i in range(total):\n",
    "\t\tcontent = f.readline().strip().split()\n",
    "\t\tif not content[0] in relation2id:\n",
    "\t\t\trelation2id[content[0]] = len(relation2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(content[0], relation2id[content[0]]))\n",
    "\tf.close()\n",
    "\tres.append(len(relation2id))\n",
    "\tf = open(data_path + \"kg/train.txt\", \"r\")\n",
    "\tfor i in f.readlines():\n",
    "\t\th, t, r = i.strip().split(\"\\t\")\n",
    "\t\tif not r in relation2id:\n",
    "\t\t\trelation2id[r] = len(relation2id)\n",
    "\t\t\tff.write(\"%s\\t%d\\n\"%(r, relation2id[r]))\n",
    "\tf.close()\n",
    "\tff.close()\n",
    "\tres.append(len(relation2id))\n",
    "\treturn res\n",
    "\n",
    "def sort_files(name, limit):\n",
    "\thash = {}\n",
    "\tf = open(data_path + \"text/\" + name + '.txt','r')\n",
    "\ts = 0\n",
    "\twhile True:\n",
    "\t\tcontent = f.readline()\n",
    "\t\tif content == '':\n",
    "\t\t\tbreak\n",
    "\t\ts = s + 1\n",
    "\t\torigin_data = content\n",
    "\t\tcontent = content.strip().split()\n",
    "\t\ten1_id = content[0]\n",
    "\t\ten2_id = content[1]\n",
    "\t\trel_name = content[4]\n",
    "\t\tif (rel_name in relation2id) and ((int)(relation2id[rel_name]) < limit[0]):\n",
    "\t\t\trelation = relation2id[rel_name]\n",
    "\t\telse:\n",
    "\t\t\trelation = relation2id['NA']\n",
    "\t\tid1 = str(en1_id)+\"#\"+str(en2_id)\n",
    "\t\tid2 = str(relation)\n",
    "\t\tif not id1 in hash:\n",
    "\t\t\thash[id1] = {}\n",
    "\t\tif not id2 in hash[id1]:\n",
    "\t\t\thash[id1][id2] = []\n",
    "\t\thash[id1][id2].append(origin_data)\n",
    "\tf.close()\n",
    "\tf = open(data_path + name + \"_sort.txt\", \"w\")\n",
    "\tf.write(\"%d\\n\"%(s))\n",
    "\tfor i in hash:\n",
    "\t\tfor j in hash[i]:\n",
    "\t\t\tfor k in hash[i][j]:\n",
    "\t\t\t\tf.write(k)\n",
    "\tf.close()\n",
    "\n",
    "def init_train_files(name, limit):\n",
    "\tprint 'reading ' + name +' data...'\n",
    "\tf = open(data_path + name + '.txt','r')\n",
    "\ttotal = (int)(f.readline().strip())\n",
    "\tsen_word = np.zeros((total, fixlen), dtype = np.int32)\n",
    "\tsen_pos1 = np.zeros((total, fixlen), dtype = np.int32)\n",
    "\tsen_pos2 = np.zeros((total, fixlen), dtype = np.int32)\n",
    "\tsen_mask = np.zeros((total, fixlen), dtype = np.int32)\n",
    "\tsen_len = np.zeros((total), dtype = np.int32)\n",
    "\tsen_label = np.zeros((total), dtype = np.int32)\n",
    "\tsen_head = np.zeros((total), dtype = np.int32)\n",
    "\tsen_tail = np.zeros((total), dtype = np.int32)\n",
    "\tinstance_scope = []\n",
    "\tinstance_triple = []\n",
    "\tfor s in range(total):\n",
    "\t\tcontent = f.readline().strip().split()\n",
    "\t\tsentence = content[5:-1]\n",
    "\t\ten1_id = content[0]\n",
    "\t\ten2_id = content[1]\n",
    "\t\ten1_name = content[2]\n",
    "\t\ten2_name = content[3]\n",
    "\t\trel_name = content[4]\n",
    "\t\tif rel_name in relation2id and ((int)(relation2id[rel_name]) < limit[0]):\n",
    "\t\t\trelation = relation2id[rel_name]\n",
    "\t\telse:\n",
    "\t\t\trelation = relation2id['NA']\n",
    "\t\ten1pos = 0\n",
    "\t\ten2pos = 0\n",
    "\t\tfor i in range(len(sentence)):\n",
    "\t\t\tif sentence[i] == en1_name:\n",
    "\t\t\t\tsentence[i] = en1_id\n",
    "\t\t\t\ten1pos = i\n",
    "\t\t\t\tsen_head[s] = word2id[en1_id]\n",
    "\t\t\tif sentence[i] == en2_name:\n",
    "\t\t\t\tsentence[i] = en2_id\n",
    "\t\t\t\ten2pos = i\n",
    "\t\t\t\tsen_tail[s] = word2id[en2_id]\n",
    "\t\ten_first = min(en1pos,en2pos)\n",
    "\t\ten_second = en1pos + en2pos - en_first\n",
    "\t\tfor i in range(fixlen):\n",
    "\t\t\tsen_word[s][i] = word2id['BLANK']\n",
    "\t\t\tsen_pos1[s][i] = pos_embed(i - en1pos)\n",
    "\t\t\tsen_pos2[s][i] = pos_embed(i - en2pos)\n",
    "\t\t\tif i >= len(sentence):\n",
    "\t\t\t\tsen_mask[s][i] = 0\n",
    "\t\t\telif i - en_first<=0:\n",
    "\t\t\t\tsen_mask[s][i] = 1\n",
    "\t\t\telif i - en_second<=0:\n",
    "\t\t\t\tsen_mask[s][i] = 2\n",
    "\t\t\telse:\n",
    "\t\t\t\tsen_mask[s][i] = 3\n",
    "\t\tfor i, word in enumerate(sentence):\n",
    "\t\t\tif i >= fixlen:\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif not word in word2id:\n",
    "\t\t\t\tsen_word[s][i] = word2id['UNK']\n",
    "\t\t\telse:\n",
    "\t\t\t\tsen_word[s][i] = word2id[word]\n",
    "\t\tsen_len[s] = min(fixlen, len(sentence))\n",
    "\t\tsen_label[s] = relation\n",
    "\t\t#put the same entity pair sentences into a dict\n",
    "\t\ttup = (en1_id,en2_id,relation)\n",
    "\t\tif instance_triple == [] or instance_triple[len(instance_triple) - 1] != tup:\n",
    "\t\t\tinstance_triple.append(tup)\n",
    "\t\t\tinstance_scope.append([s,s])\n",
    "\t\tinstance_scope[len(instance_triple) - 1][1] = s\n",
    "\t\tif (s+1) % 100 == 0:\n",
    "\t\t\tprint s\n",
    "\treturn np.array(instance_triple), np.array(instance_scope), sen_len, sen_label, sen_word, sen_pos1, sen_pos2, sen_mask, sen_head, sen_tail\n",
    "\n",
    "def init_kg():\n",
    "\tff = open(export_path + \"/triple2id.txt\", \"w\")\n",
    "\tf = open(data_path + \"kg/train.txt\", \"r\")\n",
    "\tcontent = f.readlines()\n",
    "\tff.write(\"%d\\n\"%(len(content)))\n",
    "\tfor i in content:\n",
    "\t\th,t,r = i.strip().split(\"\\t\")\n",
    "\t\tff.write(\"%d\\t%d\\t%d\\n\"%(word2id[h], word2id[t], relation2id[r]))\n",
    "\tf.close()\n",
    "\tff.close()\n",
    "\n",
    "\tf = open(export_path + \"/entity2id.txt\", \"r\")\n",
    "\tcontent = f.readlines()\n",
    "\tf.close()\n",
    "\tf = open(export_path + \"/entity2id.txt\", \"w\")\n",
    "\tf.write(\"%d\\n\"%(len(content)))\n",
    "\tfor i in content:\n",
    "\t\tf.write(i.strip()+\"\\n\")\n",
    "\tf.close()\n",
    "\n",
    "\tf = open(export_path + \"/relation2id.txt\", \"r\")\n",
    "\tcontent = f.readlines()\n",
    "\tf.close()\n",
    "\tf = open(export_path + \"/relation2id.txt\", \"w\")\n",
    "\tf.write(\"%d\\n\"%(len(content)))\n",
    "\tfor i in content:\n",
    "\t\tf.write(i.strip()+\"\\n\")\n",
    "\tf.close()\n",
    "\n",
    "textual_rel_total, rel_total = init_relation()\n",
    "entity_total, word_total = init_word()\n",
    "\n",
    "print textual_rel_total\n",
    "print rel_total\n",
    "print entity_total\n",
    "print word_total\n",
    "print word_vec.shape\n",
    "f = open(data_path + \"word2id.txt\", \"w\")\n",
    "for i in word2id:\n",
    "\tf.write(\"%s\\t%d\\n\"%(i, word2id[i]))\n",
    "f.close()\n",
    "\n",
    "init_kg()\n",
    "np.save(export_path+'vec', word_vec)\n",
    "f = open(export_path+'config', \"w\")\n",
    "f.write(json.dumps({\"word2id\":word2id,\"relation2id\":relation2id,\"word_size\":word_size, \"fixlen\":fixlen, \"maxlen\":maxlen, \"entity_total\":entity_total, \"word_total\":word_total, \"rel_total\":rel_total, \"textual_rel_total\":textual_rel_total}))\n",
    "f.close()\n",
    "sort_files(\"train\", [textual_rel_total, rel_total])\n",
    "\n",
    "# word_vec = np.load(export_path + 'vec.npy')\n",
    "# f = open(export_path + \"config\", 'r')\n",
    "# config = json.loads(f.read())\n",
    "# f.close()\n",
    "# relation2id = config[\"relation2id\"]\n",
    "# word2id = config[\"word2id\"]\n",
    "instance_triple, instance_scope, train_len, train_label, train_word, train_pos1, train_pos2, train_mask, train_head, train_tail = init_train_files(\"train_sort\",  [textual_rel_total, rel_total])\n",
    "np.save(export_path+'train_instance_triple', instance_triple)\n",
    "np.save(export_path+'train_instance_scope', instance_scope)\n",
    "np.save(export_path+'train_len', train_len)\n",
    "np.save(export_path+'train_label', train_label)\n",
    "np.save(export_path+'train_word', train_word)\n",
    "np.save(export_path+'train_pos1', train_pos1)\n",
    "np.save(export_path+'train_pos2', train_pos2)\n",
    "np.save(export_path+'train_mask', train_mask)\n",
    "np.save(export_path+'train_head', train_head)\n",
    "np.save(export_path+'train_tail', train_tail)\n",
    "print('read end...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Run the corresponding python scripts to train models: \n",
    "import os \n",
    "os.chdir('jointE')\n",
    "!bash make.sh\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import network\n",
    "import json\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "import ctypes\n",
    "import threading\n",
    "\n",
    "export_path = \"../data/\"\n",
    "\n",
    "word_vec = np.load(export_path + 'vec.npy')\n",
    "f = open(export_path + \"config\", 'r')\n",
    "config = json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "ll = ctypes.cdll.LoadLibrary   \n",
    "lib = ll(\"./init.so\")\n",
    "lib.setInPath(\"../data/\")\n",
    "lib.init()\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_float('nbatch_kg',100,'entity numbers used each training time')\n",
    "tf.app.flags.DEFINE_float('margin',1.0,'entity numbers used each training time')\n",
    "tf.app.flags.DEFINE_float('learning_rate_kg',0.001,'learning rate for kg')\n",
    "tf.app.flags.DEFINE_integer('ent_total',lib.getEntityTotal(),'total of entities')\n",
    "tf.app.flags.DEFINE_integer('rel_total',lib.getRelationTotal(),'total of relations')\n",
    "tf.app.flags.DEFINE_integer('tri_total',lib.getTripleTotal(),'total of triples')\n",
    "tf.app.flags.DEFINE_integer('katt_flag', 1, '1 for katt, 0 for att')\n",
    "\n",
    "tf.app.flags.DEFINE_string('model', 'cnn', 'neural models to encode sentences')\n",
    "tf.app.flags.DEFINE_integer('max_length',config['fixlen'],'maximum of number of words in one sentence')\n",
    "tf.app.flags.DEFINE_integer('pos_num', config['maxlen'] * 2 + 1,'number of position embedding vectors')\n",
    "tf.app.flags.DEFINE_integer('num_classes', config['textual_rel_total'],'maximum of relations')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('hidden_size',230,'hidden feature size')\n",
    "tf.app.flags.DEFINE_integer('pos_size',5,'position embedding size')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('max_epoch',30,'maximum of training epochs')\n",
    "tf.app.flags.DEFINE_integer('batch_size',160,'entity numbers used each training time')\n",
    "tf.app.flags.DEFINE_float('learning_rate',0.5,'learning rate for nn')\n",
    "tf.app.flags.DEFINE_float('weight_decay',0.00001,'weight_decay')\n",
    "tf.app.flags.DEFINE_float('keep_prob',0.5,'dropout rate')\n",
    "\n",
    "tf.app.flags.DEFINE_string('model_dir','./model/','path to store model')\n",
    "tf.app.flags.DEFINE_string('summary_dir','./summary','path to store summary_dir')\n",
    "\n",
    "\n",
    "def MakeSummary(name, value):\n",
    "\t\"\"\"Creates a tf.Summary proto with the given name and value.\"\"\"\n",
    "\tsummary = tf.Summary()\n",
    "\tval = summary.value.add()\n",
    "\tval.tag = str(name)\n",
    "\tval.simple_value = float(value)\n",
    "\treturn summary\n",
    "\n",
    "def make_shape(array,last_dim):\n",
    "\toutput = []\n",
    "\tfor i in array:\n",
    "\t\tfor j in i:\n",
    "\t\t\toutput.append(j)\n",
    "\toutput = np.array(output)\n",
    "\tif np.shape(output)[-1]==last_dim:\n",
    "\t\treturn output\n",
    "\telse:\n",
    "\t\tprint 'Make Shape Error!'\n",
    "\n",
    "def main(_):\n",
    "\n",
    "\tprint 'reading word embedding'\n",
    "\tword_vec = np.load(export_path + 'vec.npy')\n",
    "\tprint 'reading training data'\n",
    "\t\n",
    "\tinstance_triple = np.load(export_path + 'train_instance_triple.npy')\n",
    "\tinstance_scope = np.load(export_path + 'train_instance_scope.npy')\n",
    "\ttrain_len = np.load(export_path + 'train_len.npy')\n",
    "\ttrain_label = np.load(export_path + 'train_label.npy')\n",
    "\ttrain_word = np.load(export_path + 'train_word.npy')\n",
    "\ttrain_pos1 = np.load(export_path + 'train_pos1.npy')\n",
    "\ttrain_pos2 = np.load(export_path + 'train_pos2.npy')\n",
    "\ttrain_mask = np.load(export_path + 'train_mask.npy')\n",
    "\ttrain_head = np.load(export_path + 'train_head.npy')\n",
    "\ttrain_tail = np.load(export_path + 'train_tail.npy')\n",
    "\n",
    "\tprint 'reading finished'\n",
    "\tprint 'mentions \t\t: %d' % (len(instance_triple))\n",
    "\tprint 'sentences\t\t: %d' % (len(train_len))\n",
    "\tprint 'relations\t\t: %d' % (FLAGS.num_classes)\n",
    "\tprint 'word size\t\t: %d' % (len(word_vec[0]))\n",
    "\tprint 'position size \t: %d' % (FLAGS.pos_size)\n",
    "\tprint 'hidden size\t\t: %d' % (FLAGS.hidden_size)\n",
    "\treltot = {}\n",
    "\tfor index, i in enumerate(train_label):\n",
    "\t\tif not i in reltot:\n",
    "\t\t\treltot[i] = 1.0\n",
    "\t\telse:\n",
    "\t\t\treltot[i] += 1.0\n",
    "\tfor i in reltot:\n",
    "\t\treltot[i] = 1/(reltot[i] ** (0.05)) \n",
    "\tprint 'building network...'\n",
    "\tsess = tf.Session()\n",
    "\tif FLAGS.model.lower() == \"cnn\":\n",
    "\t\tmodel = network.CNN(is_training = True, word_embeddings = word_vec)\n",
    "\telif FLAGS.model.lower() == \"pcnn\":\n",
    "\t\tmodel = network.PCNN(is_training = True, word_embeddings = word_vec)\n",
    "\telif FLAGS.model.lower() == \"lstm\":\n",
    "\t\tmodel = network.RNN(is_training = True, word_embeddings = word_vec, cell_name = \"LSTM\", simple_position = True)\n",
    "\telif FLAGS.model.lower() == \"gru\":\n",
    "\t\tmodel = network.RNN(is_training = True, word_embeddings = word_vec, cell_name = \"GRU\", simple_position = True)\n",
    "\telif FLAGS.model.lower() == \"bi-lstm\" or FLAGS.model.lower() == \"bilstm\":\n",
    "\t\tmodel = network.BiRNN(is_training = True, word_embeddings = word_vec, cell_name = \"LSTM\", simple_position = True)\n",
    "\telif FLAGS.model.lower() == \"bi-gru\" or FLAGS.model.lower() == \"bigru\":\n",
    "\t\tmodel = network.BiRNN(is_training = True, word_embeddings = word_vec, cell_name = \"GRU\", simple_position = True)\n",
    "\t\n",
    "\tglobal_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "\tglobal_step_kg = tf.Variable(0,name='global_step_kg',trainable=False)\n",
    "\ttf.summary.scalar('learning_rate', FLAGS.learning_rate)\n",
    "\ttf.summary.scalar('learning_rate_kg', FLAGS.learning_rate_kg)\n",
    "\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "\tgrads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "\ttrain_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "\n",
    "\toptimizer_kg = tf.train.GradientDescentOptimizer(FLAGS.learning_rate_kg)\n",
    "\tgrads_and_vars_kg = optimizer_kg.compute_gradients(model.loss_kg)\n",
    "\ttrain_op_kg = optimizer_kg.apply_gradients(grads_and_vars_kg, global_step = global_step_kg)\n",
    "\n",
    "\tmerged_summary = tf.summary.merge_all()\n",
    "\tsummary_writer = tf.summary.FileWriter(FLAGS.summary_dir, sess.graph)\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tsaver = tf.train.Saver(max_to_keep=None)\n",
    "\tprint 'building finished'\n",
    "\n",
    "\tdef train_kg(coord):\n",
    "\t\tdef train_step_kg(pos_h_batch, pos_t_batch, pos_r_batch, neg_h_batch, neg_t_batch, neg_r_batch):\n",
    "\t\t\tfeed_dict = {\n",
    "\t\t\t\tmodel.pos_h: pos_h_batch,\n",
    "\t\t\t\tmodel.pos_t: pos_t_batch,\n",
    "\t\t\t\tmodel.pos_r: pos_r_batch,\n",
    "\t\t\t\tmodel.neg_h: neg_h_batch,\n",
    "\t\t\t\tmodel.neg_t: neg_t_batch,\n",
    "\t\t\t\tmodel.neg_r: neg_r_batch\n",
    "\t\t\t}\n",
    "\t\t\t_, step, loss = sess.run(\n",
    "\t\t\t\t[train_op_kg, global_step_kg, model.loss_kg], feed_dict)\n",
    "\t\t\treturn loss\n",
    "\n",
    "\t\tbatch_size = (FLAGS.ent_total / FLAGS.nbatch_kg)\n",
    "\t\tph = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tpt = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tpr = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tnh = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tnt = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tnr = np.zeros(batch_size, dtype = np.int32)\n",
    "\t\tph_addr = ph.__array_interface__['data'][0]\n",
    "\t\tpt_addr = pt.__array_interface__['data'][0]\n",
    "\t\tpr_addr = pr.__array_interface__['data'][0]\n",
    "\t\tnh_addr = nh.__array_interface__['data'][0]\n",
    "\t\tnt_addr = nt.__array_interface__['data'][0]\n",
    "\t\tnr_addr = nr.__array_interface__['data'][0]\n",
    "\t\tlib.getBatch.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_int]\n",
    "\t\ttimes_kg = 0\n",
    "\t\twhile not coord.should_stop():\n",
    "\t\t\ttimes_kg += 1\n",
    "\t\t\tres = 0.0\n",
    "\t\t\tfor batch in range(FLAGS.nbatch_kg):\n",
    "\t\t\t\tlib.getBatch(ph_addr, pt_addr, pr_addr, nh_addr, nt_addr, nr_addr, batch_size)\n",
    "\t\t\t\tres += train_step_kg(ph, pt, pr, nh, nt, nr)\n",
    "\t\t\ttime_str = datetime.datetime.now().isoformat()\n",
    "\t\t\tprint \"batch %d time %s | loss : %f\" % (times_kg, time_str, res)\n",
    "\n",
    "\n",
    "\tdef train_nn(coord):\n",
    "\t\tdef train_step(head, tail, word, pos1, pos2, mask, leng, label_index, label, scope, weights):\n",
    "\t\t\tfeed_dict = {\n",
    "\t\t\t\tmodel.head_index: head,\n",
    "\t\t\t\tmodel.tail_index: tail,\n",
    "\t\t\t\tmodel.word: word,\n",
    "\t\t\t\tmodel.pos1: pos1,\n",
    "\t\t\t\tmodel.pos2: pos2,\n",
    "\t\t\t\tmodel.mask: mask,\n",
    "\t\t\t\tmodel.len : leng,\n",
    "\t\t\t\tmodel.label_index: label_index,\n",
    "\t\t\t\tmodel.label: label,\n",
    "\t\t\t\tmodel.scope: scope,\n",
    "\t\t\t\tmodel.keep_prob: FLAGS.keep_prob,\n",
    "\t\t\t\tmodel.weights: weights\n",
    "\t\t\t}\n",
    "\t\t\t_, step, loss, summary, output, correct_predictions = sess.run([train_op, global_step, model.loss, merged_summary, model.output, model.correct_predictions], feed_dict)\n",
    "\t\t\tsummary_writer.add_summary(summary, step)\n",
    "\t\t\treturn output, loss, correct_predictions\n",
    "\n",
    "\t\tstack_output = []\n",
    "\t\tstack_label = []\n",
    "\t\tstack_ce_loss = []\n",
    "\n",
    "\t\ttrain_order = range(len(instance_triple))\n",
    "\n",
    "\t\tsave_epoch = 2\n",
    "\t\teval_step = 300\n",
    "\n",
    "\t\tfor one_epoch in range(FLAGS.max_epoch):\n",
    "\n",
    "\t\t\tprint('epoch '+str(one_epoch+1)+' starts!')\n",
    "\t\t\tnp.random.shuffle(train_order)\n",
    "\t\t\ts1 = 0.0\n",
    "\t\t\ts2 = 0.0\n",
    "\t\t\ttot1 = 0.0\n",
    "\t\t\ttot2 = 0.0\n",
    "\t\t\tlosstot = 0.0\n",
    "\t\t\tfor i in range(int(len(train_order)/float(FLAGS.batch_size))):\n",
    "\t\t\t\tinput_scope = np.take(instance_scope, train_order[i * FLAGS.batch_size:(i+1)*FLAGS.batch_size], axis=0)\n",
    "\t\t\t\tindex = []\n",
    "\t\t\t\tscope = [0]\n",
    "\t\t\t\tlabel = []\n",
    "\t\t\t\tweights = []\n",
    "\t\t\t\tfor num in input_scope:\n",
    "\t\t\t\t\tindex = index + range(num[0], num[1] + 1)\n",
    "\t\t\t\t\tlabel.append(train_label[num[0]])\n",
    "\t\t\t\t\tif train_label[num[0]] > 53:\n",
    "\t\t\t\t\t\tprint train_label[num[0]]\n",
    "\t\t\t\t\tscope.append(scope[len(scope)-1] + num[1] - num[0] + 1)\n",
    "\t\t\t\t\tweights.append(reltot[train_label[num[0]]])\n",
    "\t\t\t\tlabel_ = np.zeros((FLAGS.batch_size, FLAGS.num_classes))\n",
    "\t\t\t\tlabel_[np.arange(FLAGS.batch_size), label] = 1\n",
    "\t\t\t\toutput, loss, correct_predictions = train_step(train_head[index], train_tail[index], train_word[index,:], train_pos1[index,:], train_pos2[index,:], train_mask[index,:], train_len[index],train_label[index], label_, np.array(scope), weights)\n",
    "\t\t\t\tnum = 0\n",
    "\t\t\t\ts = 0\n",
    "\t\t\t\tlosstot += loss\n",
    "\t\t\t\tfor num in correct_predictions:\n",
    "\t\t\t\t\tif label[s] == 0:\n",
    "\t\t\t\t\t\ttot1 += 1.0\n",
    "\t\t\t\t\t\tif num:\n",
    "\t\t\t\t\t\t\ts1+= 1.0\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttot2 += 1.0\n",
    "\t\t\t\t\t\tif num:\n",
    "\t\t\t\t\t\t\ts2 += 1.0\n",
    "\t\t\t\t\ts = s + 1\n",
    "\n",
    "\t\t\t\ttime_str = datetime.datetime.now().isoformat()\n",
    "\t\t\t\tprint \"batch %d step %d time %s | loss : %f, NA accuracy: %f, not NA accuracy: %f\" % (one_epoch, i, time_str, loss, s1 / tot1, s2 / tot2)\n",
    "\t\t\t\tcurrent_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "\t\t\tif (one_epoch + 1) % save_epoch == 0:\n",
    "\t\t\t\tprint 'epoch '+str(one_epoch+1)+' has finished'\n",
    "\t\t\t\tprint 'saving model...'\n",
    "\t\t\t\tpath = saver.save(sess,FLAGS.model_dir+FLAGS.model+str(FLAGS.katt_flag), global_step=current_step)\n",
    "\t\t\t\tprint 'have savde model to '+path\n",
    "\n",
    "\t\tcoord.request_stop()\n",
    "\n",
    "\n",
    "\tcoord = tf.train.Coordinator()\n",
    "\tthreads = []\n",
    "\tthreads.append(threading.Thread(target=train_kg, args=(coord,)))\n",
    "\tthreads.append(threading.Thread(target=train_nn, args=(coord,)))\n",
    "\tfor t in threads: t.start()\n",
    "\tcoord.join(threads)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttf.app.run() \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "需求包 :\n",
    "TensorFlow ==1.9.0\n",
    "        Matplotlib (>=2.0.0)\n",
    "scikit-learn (>=0.18)\n",
    "scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda2.7",
   "language": "python",
   "name": "anaconda2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
